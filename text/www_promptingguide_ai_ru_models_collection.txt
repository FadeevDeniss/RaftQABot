МоделиКоллекция LLMКоллекция LLM

В данном разделе представлен сборник и краткое описание значимых и основополагающих моделей языковых моделей (LLM).
Модели
ModelRelease DateSize (B)CheckpointsDescriptionFalcon LLM (opens in a new tab)May 20237, 40Falcon-7B (opens in a new tab), Falcon-40B (opens in a new tab)Falcon LLM is a foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens. TII has now released Falcon LLM – a 40B model.PaLM 2 (opens in a new tab)May 2023--A Language Model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM.Med-PaLM 2 (opens in a new tab)May 2023--Towards Expert-Level Medical Question Answering with Large Language ModelsGorilla (opens in a new tab)May 20237Gorilla (opens in a new tab)Gorilla: Large Language Model Connected with Massive APIsRedPajama-INCITE (opens in a new tab)May 20233, 7RedPajama-INCITE (opens in a new tab)A family of models including base, instruction-tuned & chat models.LIMA (opens in a new tab)May 202365-A 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling.Replit Code (opens in a new tab)May 20233Replit Code (opens in a new tab)replit-code-v1-3b model is a 2.7B LLM trained on 20 languages from the Stack Dedup v1.2 dataset.h2oGPT (opens in a new tab)May 202312h2oGPT (opens in a new tab)h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities.CodeGen2 (opens in a new tab)May 20231, 3, 7, 16CodeGen2 (opens in a new tab)Code models for program synthesis.CodeT5 and CodeT5+ (opens in a new tab)May 202316CodeT5 (opens in a new tab)CodeT5 and CodeT5+ models for Code Understanding and Generation from Salesforce Research.StarCoder (opens in a new tab)May 202315StarCoder (opens in a new tab)StarCoder: A State-of-the-Art LLM for CodeMPT-7B (opens in a new tab)May 20237MPT-7B (opens in a new tab)MPT-7B is a GPT-style model, and the first in the MosaicML Foundation Series of models.DLite (opens in a new tab)May 20230.124 - 1.5DLite-v2-1.5B (opens in a new tab)Lightweight instruction following models which exhibit ChatGPT-like interactivity.Dolly (opens in a new tab)April 20233, 7, 12Dolly (opens in a new tab)An instruction-following LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use.StableLM (opens in a new tab)April 20233, 7StableLM-Alpha (opens in a new tab)Stability AI's StableLM series of language modelsPythia (opens in a new tab)April 20230.070 - 12Pythia (opens in a new tab)A suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters.Open Assistant (Pythia Family) (opens in a new tab)March 202312Open Assistant (opens in a new tab)OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.Cerebras-GPT (opens in a new tab)March 20230.111 - 13Cerebras-GPT (opens in a new tab)Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale ClusterBloombergGPT (opens in a new tab)March 202350-BloombergGPT: A Large Language Model for FinancePanGu-Σ (opens in a new tab)March 20231085-PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous ComputingGPT-4 (opens in a new tab)March 2023--GPT-4 Technical ReportLLaMA (opens in a new tab)Feb 20237, 13, 33, 65LLaMA (opens in a new tab)LLaMA: Open and Efficient Foundation Language ModelsChatGPT (opens in a new tab)Nov 2022--A model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.Galactica (opens in a new tab)Nov 20220.125 - 120Galactica (opens in a new tab)Galactica: A Large Language Model for SciencemT0 (opens in a new tab)Nov 202213mT0-xxl (opens in a new tab)Crosslingual Generalization through Multitask FinetuningBLOOM (opens in a new tab)Nov 2022176BLOOM (opens in a new tab)BLOOM: A 176B-Parameter Open-Access Multilingual Language ModelU-PaLM (opens in a new tab)Oct 2022540-Transcending Scaling Laws with 0.1% Extra ComputeUL2 (opens in a new tab)Oct 202220UL2, Flan-UL2 (opens in a new tab)UL2: Unifying Language Learning ParadigmsSparrow (opens in a new tab)Sep 202270-Improving alignment of dialogue agents via targeted human judgementsFlan-T5 (opens in a new tab)Oct 202211Flan-T5-xxl (opens in a new tab)Scaling Instruction-Finetuned Language ModelsAlexaTM (opens in a new tab)Aug 202220-AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq ModelGLM-130B (opens in a new tab)Oct 2022130GLM-130B (opens in a new tab)GLM-130B: An Open Bilingual Pre-trained ModelOPT-IML (opens in a new tab)Dec 202230, 175OPT-IML (opens in a new tab)OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of GeneralizationOPT (opens in a new tab)May 2022175OPT-13B (opens in a new tab), OPT-66B (opens in a new tab)OPT: Open Pre-trained Transformer Language ModelsPaLM (opens in a new tab)April 2022540-PaLM: Scaling Language Modeling with PathwaysTk-Instruct (opens in a new tab)April 202211Tk-Instruct-11B (opens in a new tab)Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP TasksGPT-NeoX-20B (opens in a new tab)April 202220GPT-NeoX-20B (opens in a new tab)GPT-NeoX-20B: An Open-Source Autoregressive Language ModelChinchilla (opens in a new tab)Mar 202270-Shows that for a compute budget, the best performances are not achieved by the largest models but by smaller models trained on more data.InstructGPT (opens in a new tab)Mar 2022175-Training language models to follow instructions with human feedbackCodeGen (opens in a new tab)Mar 20220.350 - 16CodeGen (opens in a new tab)CodeGen: An Open Large Language Model for Code with Multi-Turn Program SynthesisAlphaCode (opens in a new tab)Feb 202241-Competition-Level Code Generation with AlphaCodeMT-NLG (opens in a new tab)Jan 2022530-Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language ModelLaMDA (opens in a new tab)Jan 2022137-LaMDA: Language Models for Dialog ApplicationsGLaM (opens in a new tab)Dec 20211200-GLaM: Efficient Scaling of Language Models with Mixture-of-ExpertsGopher (opens in a new tab)Dec 2021280-Scaling Language Models: Methods, Analysis & Insights from Training GopherWebGPT (opens in a new tab)Dec 2021175-WebGPT: Browser-assisted question-answering with human feedbackYuan 1.0 (opens in a new tab)Oct 2021245-Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot LearningT0 (opens in a new tab)Oct 202111T0 (opens in a new tab)Multitask Prompted Training Enables Zero-Shot Task GeneralizationFLAN (opens in a new tab)Sep 2021137-Finetuned Language Models Are Zero-Shot LearnersHyperCLOVA (opens in a new tab)Sep 202182-What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained TransformersERNIE 3.0 Titan (opens in a new tab)July 202110-ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and GenerationJurassic-1 (opens in a new tab)Aug 2021178-Jurassic-1: Technical Details and EvaluationERNIE 3.0 (opens in a new tab)July 202110-ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and GenerationCodex (opens in a new tab)July 202112-Evaluating Large Language Models Trained on CodeGPT-J-6B (opens in a new tab)June 20216GPT-J-6B (opens in a new tab)A 6 billion parameter, autoregressive text generation model trained on The Pile.CPM-2 (opens in a new tab)Jun 2021198CPM (opens in a new tab)CPM-2: Large-scale Cost-effective Pre-trained Language ModelsPanGu-α (opens in a new tab)April 202113PanGu-α (opens in a new tab)PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel ComputationmT5 (opens in a new tab)Oct 202013mT5 (opens in a new tab)mT5: A massively multilingual pre-trained text-to-text transformerBART (opens in a new tab)Jul 2020-BART (opens in a new tab)Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and ComprehensionGShard (opens in a new tab)Jun 2020600-GShard: Scaling Giant Models with Conditional Computation and Automatic ShardingGPT-3 (opens in a new tab)May 2020175-Language Models are Few-Shot LearnersCTRL (opens in a new tab)Sep 20191.63CTRL (opens in a new tab)CTRL: A Conditional Transformer Language Model for Controllable GenerationALBERT (opens in a new tab)Sep 20190.235ALBERT (opens in a new tab)A Lite BERT for Self-supervised Learning of Language RepresentationsXLNet (opens in a new tab)Jun 2019-XLNet (opens in a new tab)Generalized Autoregressive Pretraining for Language Understanding and GenerationT5 (opens in a new tab)Oct 20190.06 - 11Flan-T5 (opens in a new tab)Exploring the Limits of Transfer Learning with a Unified Text-to-Text TransformerGPT-2 (opens in a new tab)Nov 20191.5GPT-2 (opens in a new tab)Language Models are Unsupervised Multitask LearnersRoBERTa (opens in a new tab)July 20190.125 - 0.355RoBERTa (opens in a new tab)A Robustly Optimized BERT Pretraining ApproachBERT (opens in a new tab)Oct 2018-BERT (opens in a new tab)Bidirectional Encoder Representations from TransformersGPT (opens in a new tab)June 2018-GPT (opens in a new tab)Improving Language Understanding by Generative Pre-Training
⚠️Данный раздел находится в стадии разработки.
Данные для этого раздела взяты из Papers with Code (opens in a new tab) и из недавних работ Zhao et al. (2023) (opens in a new tab).Mistral 7BРиски и неправильное использование